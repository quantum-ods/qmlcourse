{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36644785",
   "metadata": {},
   "source": [
    "(eigenvals)=\n",
    "\n",
    "# Проблема собственных значений\n",
    "\n",
    "Автор(ы):\n",
    "\n",
    "- [Синченко Семен](https://github.com/SemyonSinchenko)\n",
    "\n",
    "\n",
    "## Введение\n",
    "\n",
    "Мы с вами узнали, что задачи комбинаторной оптимизации и квантовой химии могут быть сведены к решению проблемы поиска минимального собственного значения большого эрмитова оператора -- гамильтониана. Для оптимизационных задач это [осуществляется](../../problems2qml/ru/np2ising.md) при помощи сведения к _QUBO_-матрице и гамильтониану типа Изинга. А для электронных орбиталей из квантовой химии можно [применить](../../problems2qml/ru/jordanwigner.md) преобразование Жордана-Вигнера и также перейти к спиновому гамильтониану.\n",
    "\n",
    "Теперь перед нами встает вопрос, а как же искать основное состояние этого гамильтониана? В этой лекции рассмотрим **классические** методы решения этой проблемы, то есть без квантовых компьютеров. Рассмотрение этих методов и их недостатков покажет то, зачем тут так нужен будет квантовый компьютер.\n",
    "\n",
    "## О проблеме (повторение)\n",
    "\n",
    "Эта тема обсуждалась во вводных лекциях по линейной алгебре, в [части про собственные вектора и собственные значения](../../linalg/ru/matrices.html#id9).\n",
    "\n",
    "Итак, пусть у имеется диагонализируемая матрица $A$ размерности $n \\times n$, она же является линейным оператором $\\hat{A}$. Из линейной алгебры знаем, что у этой матрицы есть $n$ таких чисел $e_i$ и векторов $\\Psi_i$, что для них выполняется условие:\n",
    "\n",
    "$$\n",
    "A \\Psi_i = e_i \\Psi_i\n",
    "$$\n",
    "\n",
    "или в нотации Дирака, которая используется в области квантовых вычислений:\n",
    "\n",
    "$$\n",
    "\\hat{A} \\ket{\\Psi_i} = e_i\\ket{\\Psi_i}\n",
    "$$\n",
    "\n",
    "Таким образом, собственные вектора -- это такие вектора, которые при применении оператора не меняют свое направление. Например, в примере ниже собственный вектор -- это ось симметрии оператора:\n",
    "\n",
    "```{figure} /_static/problems2qml/ru/eigenvals/Mona_Lisa.png\n",
    ":width: 450px\n",
    ":name: Mona_Lisa_Eigen\n",
    "\n",
    "Синий вектор, в отличии от красного, при применении оператора не меняет направление так как является его собственным вектором.\n",
    "```\n",
    "\n",
    "## Итеративные алгоритмы\n",
    "\n",
    "В целом, задача нахождения собственных значений является очень трудной с вычислительной точки зрения, особенно для больших матриц. Для матриц размера более, чем $3 \\times 3$ в общем случае не существует алгоритма нахождения собственных значений и собственных векторов. Однако существует несколько итеративных алгоритмов. Рассмотрим лишь два из них, причем без особых деталей, так как эти алгоритмы, а также доказательство их сходимости являются достаточно сложными.\n",
    "\n",
    "### Степенной метод\n",
    "\n",
    "Один из самых простых для понимания алгоритмов, который, тем не менее находит интересные применения. Суть его в том, что берем некоторый случайный вектор $\\ket{\\Psi}$ и начинаем последовательно действовать на него оператором $\\hat{A}$ (другими словами умножать, на нашу матрицу), при этом нормируя:\n",
    "\n",
    "$$\n",
    "\\ket{\\Psi_{i+1}} = \\frac{\\hat{A}\\ket{\\Psi_i}}{||\\hat{A}||}\n",
    "$$\n",
    "\n",
    "И так повторяем до тех пор, пока изменение вектора не будет меньше, чем некоторое заданное маленькое значение $\\epsilon$. Когда достигли этого условия, это значит что нашли первый собственный вектор, который соответствует наибольшему собственному значению. В частном случае интересных нам эрмитовых операторов, можно так же последовательно находить все собственные вектора и собственные значения.\n",
    "\n",
    "```{note}\n",
    "На самом деле, сеть интернета является графом -- множеством связанных между собой вершин. А любой граф можно представить в виде большой-большой, но очень разреженной матрицы, каждый элемент которой это 1 если между соответствующими вершинами есть ребро и 0, если нет. Например, элемент $L_{ij}$ будет 1, если между вершинами $i$ и $j$ есть ребро.иВ 1998-м году, Ларри Пейдж и Сергей Брин нашли очень эффективный способ подсчета первого собственного вектора этой матрицы, используя именно модификацию степенного метода. Этот алгоритм получил название `PageRank`, причем _Page_ это фамилия автора, а не отсылка к веб-страницам, как можно было бы подумать. Этот алгоритм лег в основу поисковика _Google_, который в дальнейшем вырос в транснациональную корпорацию!\n",
    "```\n",
    "\n",
    "### Итерация Арнольди\n",
    "\n",
    "Это гораздо более сложный метод, который, однако, является одним из самых эффективных применительно к разреженным матрицам {cite}`arnoldi1951`. Объяснить его легко, к сожалению, не получится, так как алгоритм требует понимания Крыловских подпространств и других концептов из области линейной алгебры разреженных систем. Но пока достаточно лишь того, что этот алгоритм имеет очень эффективную реализацию -- [ARPACK](https://www.caam.rice.edu/software/ARPACK/), написанную в середине 90-х годов на языке `FORTRAN77`. Именно эта библиотека используется \"под капотом\" у `SciPy`, а также во многих других научных пакетах. Давайте посмотрим, как она работает.\n",
    "\n",
    "Сгенерируем большую разреженную матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49112d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.random(10000)\n",
    "np.random.seed(42)\n",
    "y = np.random.random(10000)\n",
    "px = np.where(x > 0.2)\n",
    "py = np.where(y > 0.2)\n",
    "num_elements = max([px[0].shape[0], py[0].shape[0]])\n",
    "spmat = sparse.coo_matrix(\n",
    "    (\n",
    "        (np.ones(num_elements),\n",
    "        (px[0][:num_elements], py[0][:num_elements]))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(spmat.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fc80f",
   "metadata": {},
   "source": [
    "Матрица размера $10000 \\times 10000$ это большая матрица и работать с ней в \"плотном\" (dense) представлении было бы очень трудно. Но `ARPACK` позволяет найти минимальное собственное значение за доли секунд, используя разреженность матрицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25693f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import linalg as sl\n",
    "\n",
    "max_eigval = sl.eigs(spmat, k=1, which=\"LR\", return_eigenvectors=False)[0]\n",
    "min_eigval = sl.eigs(spmat, k=1, which=\"SR\", return_eigenvectors=False)[0]\n",
    "\n",
    "print(f\"Min E: {min_eigval}\\nMax E: {max_eigval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e3c32",
   "metadata": {},
   "source": [
    "Для тех кто забыл, какие параметры принимает функция `eigs` из `scipy.linalg.spare` напомним, что первый параметр это разреженная матрица, `k` -- сколько именно собственных значений хотим получить, `which` указывает на собственные значения:\n",
    "\n",
    "- `SM` -- _smallest magnitude_ -- наименьшие по модулю числа\n",
    "- `LM` -- _largest magnitude_ -- наибольшие по модулю числа\n",
    "- `SR` -- _smallers real_ -- числа с наименьшей действительной частью\n",
    "- `LR` -- _largest real_ -- числа с наибольшей действительной частью\n",
    "- `SI` -- _smallest image_ -- числа с наименьшей мнимой частью\n",
    "- `LI` -- _largest image_ -- числа с наибольшей мнимой частью\n",
    "\n",
    "Наконец, параметр `return_eigenvectors` -- хотим ли получить только собственные значения, или еще и собственные вектора.\n",
    "\n",
    "Более подробна работа с `scipy.sparse`, а также с `scipy.sparse.linalg` разбирается в [вводном блоке по линейной алгебре](пока пусто).\n",
    "\n",
    "```{note}\n",
    "Не у всех матриц все собственные значения являются действительными, поэтому `ARPACK` по умолчанию считает комплексные значения, хотя в этом конкретном случае видим, что мнимая часть равна нулю.\n",
    "```\n",
    "\n",
    "### Алгоритм Ланкзоша\n",
    "\n",
    "Итерация Ланкзоша (_англ._ Lanzos) {cite}`lanczos1950iteration` -- это модификация итерации Арнольди, которая работает с эрмитовыми матрицами и находит максимально широкое применение в том числе для квантовых гамильтонианов. Этот алгоритм по умолчанию включен в большинство математических пакетов, включая `ARPACK` и, соответственно, `SciPy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eigval = sl.eigsh(spmat, k=1, which=\"LM\", return_eigenvectors=False)[0]\n",
    "min_eigval = sl.eigsh(spmat, k=1, which=\"SM\", return_eigenvectors=False)[0]\n",
    "\n",
    "print(f\"Min E: {min_eigval}\\nMax E: {max_eigval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd327583",
   "metadata": {},
   "source": [
    "У этой процедуры из `ARPACK` немного другие варианты параметра `which`, так как мы помним, что у эрмитовых матриц собственные значения вещественны:\n",
    "\n",
    " - `LM` -- _largest magnitude_ -- наибольшие по модулю\n",
    " - `SM` -- _smallest magnitude_ -- наименьшие по модулю\n",
    " - `LA` -- _largest algebraic_ -- алгебраически наибольшие, т.е. с учетом знака\n",
    " - `SA` -- _smallest algebraic_ -- алгебраически наименьшие, т.е. с учетом знака\n",
    "\n",
    "## Вариационные алгоритмы\n",
    "\n",
    "В этом разделе поговорим о существующих алгоритмах решения задачи об основном состоянии уже в контексте квантовой механики. Хотя, как помним, задачи оптимизации и квантовой физики [тесно связаны](../../problems2qml/ru/np2ising.html#id4). В каком-то смысле, вариационные алгоритмы, а в особенности, квантовый Монте-Карло и различные его модификации в чем-то сильно похожи на классический [алгоритм имитации отжига](../../problems/ru/copt.html#id13).\n",
    "\n",
    "### Вариационный Монте-Карло\n",
    "\n",
    "**V**ariational **M**onte-**C**arlo, или просто **VMC** это очень простой и в тоже время эффективный алгоритм нахождения основного состояния квантомеханической системы.\n",
    "\n",
    "```{note}\n",
    "Замечание -- в классическом **VMC** обычно работают при нулевой температуре. Хотя в общем случае, температура оказывает значительное влияние на то, в каком состоянии находится физическая система.\n",
    "```\n",
    "\n",
    "Давайте еще раз запишем ожидаемое значение энергии гамильтониана в состоянии $\\ket{\\Psi}$:\n",
    "\n",
    "$$\n",
    "E = \\frac{\\braket{\\Psi | \\hat{H} | \\Psi}}{\\braket{\\Psi | \\Psi}}\n",
    "$$\n",
    "\n",
    "Если ввести вектор $X$, который описывает конфигурацию системы (например, ориентации спинов), то выражение для энергии можно переписать в интегральной форме:\n",
    "\n",
    "$$\n",
    "E = \\frac{\\int |\\Psi(X)|^2 \\frac{\\hat{H}\\Psi(X)}{\\Psi(X)} dX}{\\int |\\Psi(X)|^2 dX}\n",
    "$$\n",
    "\n",
    "В данном случае, выражение\n",
    "\n",
    "$$\n",
    "\\frac{|\\Psi(X)|^2}{\\int |\\Psi(X)|^2 dX}\n",
    "$$\n",
    "\n",
    "дает распределение вероятностей, а значит можно из него семплировать, используя методы Монте-Карло. Это очень похоже на то, как ранее семплировали из распределения Больцмана в [классическом методе Монте-Карло](../../problems/ru/copt.html#id13). Вопрос лишь в том, как представить волновую функцию $\\ket{\\Psi}$? В этом помогут так называемые _trial wave functions_ -- параметризированные функции от $X$. В этом случае меняем или _варьируем_ параметры _trial wave function_ в процессе:\n",
    "\n",
    "- семплируем из $\\frac{|\\Psi(X)|^2}{\\int |\\Psi(X)|^2 dX}$ конфигурации;\n",
    "- обновляем параметризацию _trial function_ так, чтобы минимизировать энергию.\n",
    "\n",
    "Повторяем до сходимости. Ну а дальше посмотрим на некоторые примеры _trial wave functions_.\n",
    "\n",
    "#### Jastrow Function\n",
    "\n",
    "Когда есть задача из $N$ квантовых частиц, каждая из которых описывается координатой или радиус вектором, то можно построить _trial wave function_ в виде суммы попарных функций двухчастичных взаимодействий:\n",
    "\n",
    "$$\n",
    "\\Psi(X) = e^{-\\sum_{i,j} u(r_i, r_j)},\n",
    "$$\n",
    "\n",
    "где $r_i, r_j$ -- это радиус-векторы частиц, а $u(r_i, r_j)$ -- симметричная функция, описывающая двухчастичное взаимодействия. Такая функция называется **Jastrow function** {cite}`jastrow1955many`. В этом случае, в процессе работы **VMC** будем просто варьировать радиус-векторы частиц также, как варьировали вершины графа в [обычном отжиге](../../problems/ru/copt.html#id13), когда решали задачу комбинаторной оптимизации. Только теперь есть еще и параметризация обменных взаимодействий, которую \"варьируем\".\n",
    "\n",
    "#### Hartree-Fock (SCF)\n",
    "\n",
    "Для задач квантовой химии, когда работаем с фермионами, существует вид _trial wave function_ на основе Слэтеровского детерминанта, о котором  писали в [продвинутой лекции по квантовой химии](../../problems/ru/quantchemadvancedscf.html#id13):\n",
    "\n",
    "$$\n",
    "\\Psi(R) = D^{\\uparrow}D^{\\downarrow},\n",
    "$$\n",
    "\n",
    "где $D$ это матрица из одноэлектронных орбиталей:\n",
    "\n",
    "$$\n",
    "D = \\begin{bmatrix}\n",
    "  \\psi_1(r_1) & \\psi_1(r_2) & ... & \\psi_1(r_{N / 2}) \\\\\n",
    "  ... & ... & ... & ... \\\\\n",
    "  \\psi_{N / 2}(r_1) & \\psi_{N / 2}(r_2) & ... & \\psi_{N / 2}(r_{N / 2}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Jastrow Function для спинов\n",
    "\n",
    "Дальше нас будут интересовать как раз модели Изинга и спины, а не частицы в пространстве или орбитали из вторичного квантования. Для спинов можем записать Jastrow function следующим образом:\n",
    "\n",
    "$$\n",
    "\\Psi(s) = e^{\\sum_{i,j}s_i W_{i,j} s_j},\n",
    "$$\n",
    "\n",
    "где матрица $W$ будет играть роль параметризации и отражать парные спиновые корреляции. Давайте посмотрим это на практике при помощи библиотеки `NetKet` {cite}`carleo2019netket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket as nk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b38be",
   "metadata": {},
   "source": [
    "Моделировать будем простую [модель Изинга](../../problems/ru/ising.md) для цепочки из 10 спинов (чтобы быстро считалось):\n",
    "\n",
    "$$\n",
    "\\hat{H} = -h \\sum_i \\sigma^x_i + J \\sum_{i,j}\\sigma^z_i \\sigma^z_j\n",
    "$$\n",
    "\n",
    "Параметры возьмем такими:\n",
    "- $J=0.5$\n",
    "- $h=1.321$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nk.graph.Hypercube(length=10, n_dim=1, pbc=True)\n",
    "hi = nk.hilbert.Spin(s=0.5, N=g.n_nodes)\n",
    "op = nk.operator.Ising(h=1.321, hilbert=hi, J=0.5, graph=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3393e41",
   "metadata": {},
   "source": [
    "Поскольку модель относительно небольшая по числу частиц, то сразу можем получить точное решение методом Ланкзоша."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a80bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact = nk.exact.lanczos_ed(op)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357a4f8",
   "metadata": {},
   "source": [
    "Создадим модель на основе Jastrow и **VMC**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = nk.sampler.MetropolisLocal(hi)\n",
    "model = nk.models.Jastrow(dtype=complex)\n",
    "optimizer = nk.optimizer.Sgd(learning_rate=0.05)\n",
    "sr = nk.optimizer.SR(diag_shift=0.01)\n",
    "vmc = nk.driver.VMC(op, optimizer, sampler, model, n_samples=1008, preconditioner=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166acbf",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Изучение документации библиотеки `NetKet` оставляем вам самим, так как объяснение абстракций графа и гильбертова пространства, а также использование метода `stochastic reconfiguration` для вычисления градиентов выходит за рамки лекции. Документаци представлена на [сайте NetKet](https://www.netket.org).\n",
    "```\n",
    "\n",
    "Запустим оптимизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fe124",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = nk.logging.RuntimeLog()\n",
    "vmc.run(50, out=logger, show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1321d26",
   "metadata": {},
   "source": [
    "Посмотрим на результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(list(range(50)), np.real(logger.data[\"Energy\"][\"Mean\"]), \".-\", label=\"VMC mean energy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.hlines(exact, 0, 50, label=\"Exact solution\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04684673",
   "metadata": {},
   "source": [
    "#### Neural Network Quantum States\n",
    "\n",
    "Еще более интересный подход к выбору _trial wave function_ -- это использование в качестве $\\Psi(X)$ нейронной сети {cite}`carleo2017nqs`. Уже немного [касались этой темы](../../qmlkinds/ru/qmlkinds.html#nqs), когда речь шла о видах квантового машинного обучения. Хороший вариант, это использовать, например, полносвязную сеть -- ограниченную машину Больцмана:\n",
    "\n",
    "```{figure} /_static/problems2qml/ru/eigenvals/NQS.png\n",
    ":width: 450px\n",
    ":name: NQS2\n",
    "Нейронная сеть в качестве _trial wave function_ из работы {cite}`carleo2017nqs`.\n",
    "```\n",
    "\n",
    "Это также легко может быть реализовано с использованием библиотеки `NetKet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nk.models.RBM()\n",
    "optimizer = nk.optimizer.Sgd(learning_rate=0.05)\n",
    "sr = nk.optimizer.SR(diag_shift=0.01)\n",
    "vmc = nk.driver.VMC(op, optimizer, sampler, model, n_samples=1000, preconditioner=sr)\n",
    "\n",
    "logger = nk.logging.RuntimeLog()\n",
    "vmc.run(50, out=logger, show_progress=False)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(list(range(50)), np.real(logger.data[\"Energy\"][\"Mean\"]), \".-\", label=\"VMC mean energy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.hlines(exact, 0, 50, label=\"Exact solution\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e71b8",
   "metadata": {},
   "source": [
    "Преимущества использования нейронной сети трудно показать на таком небольшом примере с моделью Изинга и 10-ю спинами, но они полностью раскрываются, если нужно анализировать более сложные модели.\n",
    "\n",
    "```{note}\n",
    "Это интересно, но при помощи библиотеки `NetKet` можно по сути решать проблемы комбинаторной оптимизации {cite}`sinchenko2019deep` с помощью методов _deep learning_.\n",
    "```\n",
    "\n",
    "### Проблемы с VMC\n",
    "\n",
    "К сожалению, у метода **VMC** есть свои проблемы. Это относительно плохая масштабируемость -- при росте размерности проблемы для того, чтобы подобрать реально хорошую аппроксимацию потребуется все больше итераций и семплов на каждой из них. Также у **VMC** есть ряд фундаментальных проблем, например, так называемая _sign problem_ {cite}`loh1990sign`.\n",
    "\n",
    "## Заключение\n",
    "\n",
    "В этой лекции рассмотрены известные подходы к решению задачи о минимальном собственном значении на классическом компьютере. Как увидели, все эти методы не могут быть масштабированы на реально большие операторы. Так что для решения этих проблем действительно нужен квантовый компьютер."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   11,
   77,
   96,
   100,
   107,
   130,
   135,
   219,
   221,
   233,
   237,
   241,
   243,
   247,
   253,
   261,
   264,
   268,
   278,
   292,
   308
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}