{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5043a467",
   "metadata": {},
   "source": [
    "(classicsvm)=\n",
    "\n",
    "# Классический SVM\n",
    "\n",
    "Автор(ы):\n",
    "\n",
    "- [Кашницкий Юрий](https://github.com/yorko)\n",
    "\n",
    "\n",
    "## Описание лекции\n",
    "\n",
    "В этой лекции мы рассмотрим классический метод опорных векторов (SVM) и разберем стоящую за ним математику. С согласия Евгения Соколова, мы во многом переиспользуем конспекты лекций [курса](https://github.com/esokolov/ml-course-hse) \"Машинное обучение\", читаемого на ФКН ВШЭ.\n",
    "\n",
    "План лекции такой:\n",
    "\n",
    "- линейная классификация;\n",
    "- интуиция метода опорных векторов;\n",
    "- метод опорных векторов для линейно-разделимой выборки;\n",
    "- метод опорных векторов для линейно-неразделимой выборки;\n",
    "- решение задачи метода опорных векторов;\n",
    "- ядерный переход;\n",
    "- плюсы и минусы SVM.\n",
    "\n",
    "## Линейная классификация\n",
    "\n",
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на два полупространства, в каждом из которых прогнозируется одно из двух значений целевого класса. Если это можно сделать без ошибок, то обучающая выборка называется _линейно разделимой_.\n",
    "\n",
    "```{figure} /_static/qsvmblock/lin_classifier.png\n",
    ":width: 350px\n",
    ":name: lin_classifier\n",
    "\n",
    "Линейная классификация\n",
    "```\n",
    "\n",
    "Рассмотрим задачу бинарной классификации, в которой $\\mathbb{X} = \\mathbb{R}^d$ -- пространство объектов,\n",
    "$Y = \\{-1, +1\\}$ -- множество допустимых ответов (целевой признак),\n",
    "$X = \\{(x_i, y_i)\\}_{i = 1}^\\ell$ -- обучающая выборка. Будем класс $+1$ называть положительным, а класс $-1$ -- отрицательным. Здесь $d$ -- размерность признакового пространства, $\\ell$ -- количество примеров в обучающей выборке.\n",
    "\n",
    "__Линейная модель классификации__ определяется следующим образом:\n",
    "\n",
    "$$\n",
    "    a(x) =\n",
    "    \\text{sign} \\left(\n",
    "        \\langle w, x \\rangle + b\n",
    "    \\right)\n",
    "    =\n",
    "    \\text{sign} \\left(\n",
    "        \\sum_{j = 1}^{d} w_j x_j + b\n",
    "    \\right),\n",
    "$$\n",
    "\n",
    "где $w \\in \\mathbb{R}^d$ -- вектор весов, $b \\in \\mathbb{R}$ -- сдвиг (bias), $\\text{sign}(\\cdot)$ – функция \"сигнум\", возвращающая знак своего аргумента, $a(x)$ – ответ классификатора на примере $x$.\n",
    "\n",
    "```{note}\n",
    "\n",
    "Хорошо бы для векторов ставить стрелку и писать $\\vec{w}$, но мы этого не будем делать, предполагая, что из контекста ясно, что вектор, а что скаляр. В частности, в формуле выше $w$ и $x$ -- вектора, а $a(x), w_j, x_j, d$ и $b$ -- скаляры.\n",
    "```\n",
    "\n",
    "Часто считают, что среди признаков\n",
    "есть константа, $x_{d + 1} = 1$.\n",
    "В этом случае нет необходимости вводить сдвиг $b$,\n",
    "и линейный классификатор можно задавать как\n",
    "\n",
    "$$\n",
    "    a(x) = \\text{sign} \\langle w, x \\rangle.\n",
    "$$\n",
    "\n",
    "\n",
    "```{figure} /_static/qsvmblock/distance_to_the_plane.png\n",
    ":width: 400px\n",
    ":name: distance_to_the_plane\n",
    "\n",
    "Расстояние от точки до плоскости\n",
    "```\n",
    "\n",
    "Геометрически линейный классификатор соответствует гиперплоскости с вектором нормали $w$, которая задается уравнением $\\langle w, x \\rangle = 0$. Величина скалярного произведения $\\langle w, x \\rangle$ пропорциональна расстоянию от гиперплоскости до точки $x$, а его знак показывает, с какой стороны от гиперплоскости находится данная точка. Если быть точным, расстояние от точки с радиус-вектором $x_A$ до плоскости $\\langle w, x \\rangle = 0$:\n",
    "\n",
    "$$\n",
    "\\rho(x_A, \\langle w, x \\rangle = 0) = \\frac{\\langle w, x_A \\rangle}{||w||}.\n",
    "$$\n",
    "\n",
    "```{admonition} Упражнение на метод Лагранжа\n",
    "\n",
    "[Метод Лагранжа](https://en.wikipedia.org/wiki/Lagrange_multiplier) -- очень важный метод оптимизации функций при наличии органичений. Этот же метод вовсю используется ниже в методе опорных векторов.\n",
    "\n",
    "Рекомендуется ознакомиться с методом Лагранжа и решить (ручками!) простую оптимизационную задачу вида\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & x + y \\to \\min_{x,y} \\\\\n",
    "            & x^2 + y^2 = 1\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "А мы выведем указанную выше формулу расстояния от точки до плоскости. Вообще это можно сделать разными способами -- алгебраически и геометрически. Но давайте посмотрим на эту задачу (внезапно) как на задачу оптимизации и решим ее методом Лагранжа. Это послужит неплохой тренировкой перед тем как окунуться в SVM.\n",
    "\n",
    "Итак, представим задачу в таком виде: хотим найти точку $x$ на плоскости $\\langle w, x \\rangle = 0$ такую, что расстояние от $x$ до точки $x_0$ минимально:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\rho(x, x_0) \\to \\min_{x} \\\\\n",
    "            & \\langle w, x \\rangle = 0\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "Лагранжиан: $\\mathcal{L}(x, \\lambda) = {||x - x_0||}^2 + 2 \\lambda \\langle w, x \\rangle$. Тут мы для $\\rho(x, x_0)$ подставили квадрат расстояния ${||x - x_0||}^2$ (такой переход от росстояния к его квадрат хорошо бы обосновать монотонностью оптимизируемой функции, но мы это опустим), а также перед коэффициентом $\\lambda$ для удобства поставили 2, что несущественно.\n",
    "\n",
    "Далее надо приравнять нулю частные производные лагранжиана по его аргументам. Из\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial x} = 0$ получаем: $2(x - x_0) + 2 \\lambda w = 0 \\Rightarrow x = x_0 - \\lambda w$.\n",
    "\n",
    "Теперь домножим это уравнение скалярно на $w$ и выразим $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\langle w, x \\rangle  = \\langle w, x_0 \\rangle - \\lambda {||w||}^2 \\Rightarrow \\lambda = \\frac{\\langle w, x_0 \\rangle}{{||w||}^2 }\n",
    "$$\n",
    "\n",
    "Тогда наконец\n",
    "\n",
    "$$\n",
    "\\min_{x,  \\langle w, x \\rangle = 0} \\rho(x, x_0) = ||x - x_0|| = ||(x_0 - \\lambda w) - x_0|| = |\\lambda| ||w|| = \\frac{\\langle w, x_0 \\rangle}{||w||}\\ \\ \\square.\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "Таким образом, линейный классификатор разделяет пространство на две части с помощью гиперплоскости, и при этом одно полупространство относит к положительному классу, а другое -- к отрицательному.\n",
    "\n",
    "Пожалуй, самый известный и популярный на практике представитель семейства линейных классификаторов -- логистическая регрессия. На русском языке про нее можно почитать в [статье](https://habr.com/ru/company/ods/blog/323890/) открытого курса по машинному обучению или в упомянутых [лекциях](https://github.com/esokolov/ml-course-hse/tree/master/2021-fall/lecture-notes) Евгения Соколова. В этих лекциях также объясняется, как происходит обучение модели (подбор весов $w$) за счет минимизации функции потерь.\n",
    "\n",
    "### Отступ классификации\n",
    "\n",
    "Оказывается полезным рассмотреть выражение $M(x_i, y_i, w) = y_i \\cdot \\langle w, x \\rangle.$\n",
    "\n",
    "Это __отступ классификации__ (margin) для объекта обучающей выборки $(x_i, y_i)$. К сожалению, его очень легко перепутать с зазором классификации, который появится чуть ниже в изложении интуиции метода опорных векторов. Чтобы не путать: отступ определен на конкретном объекте обучающей выборки.\n",
    "\n",
    "\n",
    "```{figure} /_static/qsvmblock/margin_toy_example.png\n",
    ":width: 400px\n",
    ":name: margin_toy_example\n",
    "\n",
    "Иллюстрация к понятию отступа классификации\n",
    "```\n",
    "\n",
    "Отступ -- это своего рода \"уверенность\" модели в классификации объекта $(x_i, y_i)$:\n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделяющей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделяющей гиперплоскости, а знак отступа определяет, правильно ли объект классифицирован. На рисунке – $x_2$ и $x_4$.\n",
    "\n",
    "Далее увидим, что понятие отступа классификации -- часть функции потерь, которая оптимизируется в методе опорных векторов.\n",
    "\n",
    "## Интуиция метода опорных векторов\n",
    "\n",
    "Метод опорных векторов (Support Vector Machine, SVM) основан на идее максимизации зазора между классами. Пока не вводим этот термин формально, но передадим интуицию метода. На Рис. {numref}`linclass_margins` показана линейно-разделимая выборка, кружки соответствуют положительным примерам, а квадраты -- отрицательным (или наоборот), а оси -- некоторым признакам этих примеров. На рисунке слева показаны две прямые (в общем случае -- гиперплоскости), разделяющие выборку. Кажется, что синяя прямая лучше тем, что она дальше отстоит от примеров обучающей выборки, чем красная прямая (зазор -- больше), и потому лучше будет разделять другие примеры из того же распределения, что и примеры обучающей выборки. То есть такой линейный классификатор будет лучше обобщаться на новые данные. Теория подтверждает описанную интуицию {cite}`MohriRostamizadehTalwalkar18`.\n",
    "\n",
    "\n",
    "```{figure} /_static/qsvmblock/linclass_margins.png\n",
    ":width: 450px\n",
    ":name: linclass_margins\n",
    "\n",
    "Слева показаны две прямые (в общем случае -- гиперплоскости), разделяющие выборку. Справа показана прямая, максимизирующая зазор между классами. Источник: [лекция](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html) Cornell\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Одним из ключевых авторов алгоритма SVM является Владимир Вапник -- советский и американский (с 1991-го года) ученый, который также сделал огромный вклад в теорию классического машинного обучения. Его имя носит одно из ключевых теоретических понятий машинного обучения -- размерность Вапника-Червоненкиса.\n",
    "```\n",
    "\n",
    "## Метод опорных векторов для линейно-разделимой выборки\n",
    "\n",
    "Будем рассматривать линейные классификаторы вида\n",
    "\n",
    "$$ a(x) = \\text{sign} (\\langle w, x \\rangle + b), \\qquad w \\in \\mathbb{R}^d, b \\in \\mathbb{R}. $$\n",
    "\n",
    "Заметьте, что мы вернули сдвиг (bias) $b$. Будем считать, что существуют такие параметры $w_*$ и $b_*$, что соответствующий им классификатор $a(x)$ не допускает ни одной ошибки на обучающей выборке. В этом случае говорят, что выборка __линейно разделима__.\n",
    "\n",
    "\n",
    "Заметим, что если одновременно умножить параметры $w$ и $b$ на одну и ту же положительную константу, то классификатор не изменится. Распорядимся этой свободой выбора и отнормируем параметры так, что\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X} | \\langle w, x \\rangle + b| = 1.\n",
    "$$\n",
    "\n",
    "Как мы увидели выше, расстояние от произвольной точки $x_0 \\in \\mathbb{R}^d$ до гиперплоскости, определяемой данным классификатором, равно\n",
    "\n",
    "$$\n",
    "    \\rho(x_0, a)\n",
    "    =\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }.\n",
    "$$\n",
    "\n",
    "Тогда расстояние от гиперплоскости до ближайшего примера из обучающей выборки равно\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X}\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|} \\min_{x \\in X} |\\langle w, x \\rangle + b|\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "Данная величина также называется __зазором__ (margin). Опять же, эту величину очень легко перепутать с отступом классификации, про который мы говорили выше и который тоже margin в англоязычном варианте. Заметим, что отступ -- функция параметров $w$ и конкретного примера обучающей выборки $(x_i, y_i)$, а зазор – функция только параметров $w$ при описанном трюке с нормировкой $w$ и $b$ (в противном случае зазор -- также функция сдвига $b$ и всех примеров $x$).\n",
    "\n",
    "Таким образом, если классификатор без ошибок разделяет обучающую выборку, то ширина его разделяющей полосы равна $\\frac{2}{\\|w\\|}$.\n",
    "Максимизация ширины разделяющей полосы приводит\n",
    "к повышению обобщающей способности классификатора {cite}`MohriRostamizadehTalwalkar18 `. На повышение обобщающей способности направлена и регуляризация,\n",
    "которая штрафует большую норму весов -- а чем больше норма весов,\n",
    "тем меньше ширина разделяющей полосы.\n",
    "\n",
    "Итак, требуется построить классификатор, идеально разделяющий обучающую выборку, и при этом имеющий максимальный отступ.\n",
    "\n",
    "Запишем соответствующую оптимизационную задачу,\n",
    "которая и будет определять метод опорных векторов для линейно разделимой выборки (hard margin support vector machine):\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin{aligned}\n",
    "        & \\frac{1}{2} \\|w\\|^2 \\to \\min_{w, b} \\\\\n",
    "        & y_i \\left(\n",
    "            \\langle w, x_i \\rangle + b\n",
    "        \\right) \\geq 1, \\quad i = 1, \\dots, \\ell.\n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Здесь мы воспользовались тем, что линейный классификатор дает правильный ответ на примере $x_i$ тогда и только тогда, когда $y_i (\\langle w, x_i \\rangle + b) \\geq 0$ (вспомним, что $M_i = y_i (\\langle w, x_i \\rangle + b)$ -- отступ классификации для примера $(x_i, y_i)$ обучающей выборки). Более того, из условия нормировки $\\min_{x \\in X} | \\langle w, x \\rangle + b| = 1$ следует, что $y_i (\\langle w, x_i \\rangle + b) \\geq 1$.\n",
    "\n",
    "В данной задаче функционал является строго выпуклым, а ограничения линейными, поэтому сама задача является выпуклой и имеет единственное решение. Более того, задача является квадратичной и может быть решена крайне эффективно.\n",
    "\n",
    "## Метод опорных векторов для линейно-неразделимой выборки\n",
    "\n",
    "Рассмотрим теперь общий случай, когда выборку\n",
    "невозможно идеально разделить гиперплоскостью.\n",
    "Это означает, что какие бы $w$ и $b$ мы ни взяли,\n",
    "хотя бы одно из ограничений в предыдущей задаче\n",
    "будет нарушено:\n",
    "\n",
    "$$\n",
    "    \\exists \\ x_i \\in X:\\\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) < 1.\n",
    "$$\n",
    "\n",
    "Сделаем эти ограничения _мягкими_, введя штраф $\\xi_i \\geq 0$ за их нарушение:\n",
    "\n",
    "$$\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell.\n",
    "$$\n",
    "\n",
    "Отметим, что если отступ объекта лежит между нулем и\n",
    "единицей ($0 \\leq y_i \\left( \\langle w, x_i \\rangle + b \\right) < 1$), то объект верно классифицируется, но имеет ненулевой штраф $\\xi > 0$. Таким образом, мы штрафуем объекты за попадание внутрь разделяющей полосы.\n",
    "\n",
    "Величина $\\frac{1}{\\|w\\|}$ в данном случае называется __мягким зазором__ (soft margin). С одной стороны, мы хотим максимизировать зазор, с другой -- минимизировать\n",
    "штраф за неидеальное разделение выборки $\\sum_{i = 1}^{\\ell} \\xi_i$.\n",
    "\n",
    "Эти две задачи противоречат друг другу: как правило, излишняя подгонка под выборку приводит к маленькому зазору, и наоборот -- максимизация зазора приводит к большой ошибке на обучении.\n",
    "В качестве компромисса будем минимизировать взвешенную сумму двух указанных величин.\n",
    "\n",
    "Приходим к оптимизационной задаче, соответствующей методу опорных векторов для линейно неразделимой выборки (soft margin support vector machine):\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin{aligned}\n",
    "        & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, b, \\xi} \\\\\n",
    "        & y_i \\left(\n",
    "            \\langle w, x_i \\rangle + b\n",
    "        \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "        & \\xi_i \\geq 0, \\quad i = 1, \\dots, \\ell.\n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Чем больше здесь параметр $C$, тем сильнее мы будем настраиваться на обучающую выборку. Данная задача также является выпуклой и имеет единственное решение.\n",
    "\n",
    "### Сведение к безусловной задаче оптимизации\n",
    "\n",
    "Покажем, что задачу метода опорных векторов можно свести к задаче безусловной оптимизации функционала, который имеет вид верхней оценки на долю неправильных ответов.\n",
    "\n",
    "Перепишем условия задачи:\n",
    "\n",
    "$$\n",
    "    \\left\\{\n",
    "    \\begin{aligned}\n",
    "        &\\xi_i \\geq 1 - y_i (\\langle w, x_i \\rangle + b) \\\\\n",
    "        &\\xi_i \\geq 0\n",
    "    \\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Поскольку при этом в функционале требуется, чтобы штрафы $\\xi_i$ были как можно меньше, то можно получить следующую явную формулу для них:\n",
    "\n",
    "$$\n",
    "    \\xi_i\n",
    "    =\n",
    "    \\max(0,\n",
    "        1 - y_i (\\langle w, x_i \\rangle + b)).\n",
    "$$\n",
    "\n",
    "Данное выражение для $\\xi_i$ уже учитывает в себе все ограничения задачи, описанной выше.\n",
    "Значит, если подставить его в функционал, получим безусловную задачу оптимизации:\n",
    "\n",
    "$$\n",
    "    \\frac{1}{2} \\|w\\|^2\n",
    "    +\n",
    "    C\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\max(0,\n",
    "            1 - y_i (\\langle w, x_i \\rangle + b))\n",
    "    \\to\n",
    "    \\min_{w, b}\n",
    "$$\n",
    "\n",
    "Эта задача является негладкой, поэтому решать её может быть достаточно тяжело. Тем не менее, она показывает, что метод опорных векторов, по сути, как и логистическая регрессия, строит верхнюю оценку на долю ошибок и добавляет к ней стандартную квадратичную регуляризацию. Только если в случае логистической регрессии этой верхней оценкой была логистическая функция потерь (опять сделаем отсылку к [статье](https://habr.com/ru/company/ods/blog/323890/) из открытого курса машинного обучения), то в случае метода опорных векторов это функция вида  $L(y, z) = \\max(0, 1 - yz)$, которая называется __кусочно-линейной функцией потерь (hinge loss)__.\n",
    "\n",
    "```{figure} /_static/qsvmblock/loss_functions_logistic_and_hinge.png\n",
    ":width: 450px\n",
    ":name: loss_functions_logistic_and_hinge\n",
    "\n",
    "Пороговая, кусочно-линейная и логистическая функции потерь\n",
    "```\n",
    "\n",
    "Это становится понятнее в терминах упомянутого выше отступа $M_i = y_i (\\langle w, x_i \\rangle + b)$ на примере обучающей выборки. В идеале мы хотели бы штрафовать классификатор за ошибку на примере: $L_{1/0} (M_i) = [M_i < 0]$. Это пороговая функция потерь (zero-one loss), ее график изображен черным на {numref}`loss_functions_logistic_and_hinge` как функция от отступа. К сожалению, напрямую мы не можем эффективно оптимизировать такую функцию градиентными методами из-за разрыва в нуле, поэтому оптимизируется верхняя оценка zero-one loss. В случае логистической регрессии -- логистическая функция потерь $L(M_i) = \\log(1+ e^{-M_i})$ (красная на рисунке выше), а в случае метода опорных векторов -- кусочно-линейная функция $L(M_i) = \\max(0, 1 - M_i)$ (зеленая на рисунке выше).\n",
    "\n",
    "```{note}\n",
    "Бытует мнение, что метод опорных векторов сегодня нигде не используется из-за его сложности (как минимум квадратичной по числу примеров). Однако, это не так. Линейный SVM вполне неплохо можно применять в задачах с высокой размерностью объектов обучающей выборки, например, для классификации текстов с [Tf-Idf](https://en.wikipedia.org/wiki/Tf–idf) или любым другим разреженным представлением. В частности, [Vowpal Wabbit](https://vowpalwabbit.org/) -- очень эффективная утилита для решения многих задачах машинного обучения -- по умолчанию использует hinge-loss для задач классификации, то есть по сути в этом сценарии применения является линейным SVM. Кусочно-линейная функция потерь хороша тем, что у нее очень простая производная – положительная константа либо ноль. Это удобно использовать с SGD и большими выборками, когда приходится делать миллиарды обновлений весов.\n",
    "\n",
    "Про прелести Vowpal Wabbit и обучение на гигабайтах данных за считанные минуты можно почитать в [статье](https://habr.com/ru/company/ods/blog/326418/) открытого курса машинного обучения.\n",
    "```\n",
    "\n",
    "## Решение задачи метода опорных векторов\n",
    "\n",
    "Итак, метод опорных векторов сводится к решению задачи оптимизации\n",
    "\n",
    "$$\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, b, \\xi} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\xi_i \\geq 0, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Для решения таких _условных_ задач оптимизации с условиями в виде неравенств или равенств часто используют лагранжиан и двойственную задачу оптимизации. Этот подход исчерпывающе описан в классической книге Бойда по оптимизации {cite}`Boyd2006`, а на русском языке можно обратиться к [конспекту](http://www.machinelearning.ru/wiki/images/f/fe/Sem6_linear.pdf) Евгения Соколова. Также для понимания материала рекомендуется рассмотреть [\"игрушечный\" пример](https://github.com/esokolov/ml-course-hse/blob/master/2016-spring/seminars/sem16-svm.pdf) решения задачи метода опорных векторов в случае линейно-разделимой выборки из 5 примеров.\n",
    "\n",
    "Построим двойственную задачу к задаче метода опорных векторов.\n",
    "Запишем лагранжиан:\n",
    "\n",
    "$$\n",
    "    L(w, b, \\xi, \\lambda, \\mu)\n",
    "    =\n",
    "    \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i\n",
    "    -\n",
    "    \\sum_{i = 1}^{\\ell} \\lambda_i \\left[\n",
    "        y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) - 1 + \\xi_i\n",
    "    \\right]\n",
    "    -\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\mu_i \\xi_i.\n",
    "$$\n",
    "\n",
    "Выпишем условия Куна-Таккера:\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond1\n",
    "\n",
    "\\nabla_w L = w - \\sum_{i = 1}^{\\ell} \\lambda_i y_i x_i = 0\n",
    "  \\quad\\Longrightarrow\\quad\n",
    "    w = \\sum_{i = 1}^{\\ell} \\lambda_i y_i x_i\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond2\n",
    "\n",
    "\\nabla_b L = - \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond3\n",
    "\n",
    "  \\nabla_{\\xi_i} L = C - \\lambda_i - \\mu_i\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    \\lambda_i + \\mu_i = C\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond4\n",
    "\n",
    "\\lambda_i \\left[\n",
    "        y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) - 1 + \\xi_i\n",
    "        \\right] = 0\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    (\\lambda_i = 0)\n",
    "        \\ \\text{или}\\\n",
    "        \\left(\n",
    "            y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right)\n",
    "            =\n",
    "            1 - \\xi_i\n",
    "        \\right)\n",
    "```\n",
    "\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond5\n",
    "\n",
    "\\mu_i \\xi_i = 0\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    (\\mu_i = 0)\n",
    "        \\ \\text{или}\\\n",
    "        (\\xi_i = 0)\n",
    "```\n",
    "\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond6\n",
    "\n",
    "    \\xi_i \\geq 0, \\lambda_i \\geq 0, \\mu_i \\geq 0.\n",
    "```\n",
    "\n",
    "Проанализируем полученные условия. Из {eq}`KuckTuckerCond1` следует, что вектор весов, полученный в результате настройки SVM, можно записать как линейную комбинацию объектов, причем веса в этой линейной комбинации можно найти как решение двойственной задачи.\n",
    "\n",
    "В зависимости от значений $\\xi_i$ и $\\lambda_i$ объекты $x_i$ разбиваются на три категории:\n",
    "\n",
    "- $\\xi_i = 0$, $\\lambda_i = 0$. Такие объекты не влияют решение $w$ (входят в него с нулевым весом $\\lambda_i$), правильно классифицируются ($\\xi_i = 0$) и лежат вне разделяющей полосы. Объекты этой категории называются _периферийными_.\n",
    "- $\\xi_i = 0$, $0 < \\lambda_i < C$. Из условия {eq}`KuckTuckerCond4` следует, что $y_i \\left(\\langle w, x_i \\rangle + b \\right) = 1$, то есть объект лежит строго на границе разделяющей полосы. Поскольку $\\lambda_i > 0$, объект влияет на решение $w$. Объекты этой категории называются _опорными граничными_.\n",
    "- $\\xi_i > 0$, $\\lambda_i = C$. Такие объекты могут лежать внутри разделяющей полосы ($0 < \\xi_i < 2$) или выходить за ее пределы ($\\xi_i \\geq 2$). При этом если $0 < \\xi_i < 1$, то объект классифицируется правильно, в противном случае -- неправильно. Объекты этой категории называются _опорными нарушителями_.\n",
    "\n",
    "Отметим, что варианта $\\xi_i > 0$, $\\lambda_i < C$ быть не может, поскольку при $\\xi_i > 0$ из условия дополняющей нежесткости {eq}`KuckTuckerCond5` следует, что $\\mu_i = 0$, и отсюда из уравнения {eq}`KuckTuckerCond3` получаем, что $\\lambda_i = C$.\n",
    "\n",
    "Итак, итоговый классификатор зависит только от объектов, лежащих на границе разделяющей полосы, и от объектов-нарушителей (с $\\xi_i > 0$).\n",
    "\n",
    "Построим двойственную функцию. Для этого подставим выражение {eq}`KuckTuckerCond1` в лагранжиан и воспользуемся уравнениями {eq}`KuckTuckerCond2` и {eq}`KuckTuckerCond3` (данные\n",
    "три уравнения выполнены для точки минимума лагранжиана при\n",
    "любых фиксированных $\\lambda$ и $\\mu$):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    L &= \\frac{1}{2} \\left\\|\n",
    "            \\sum_{i = 1}^{\\ell}\n",
    "                \\lambda_i y_i x_i\n",
    "        \\right\\|^2\n",
    "        -\n",
    "        \\sum_{i, j = 1}^{\\ell}\n",
    "            \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle\n",
    "        -\n",
    "        b\n",
    "        \\underbrace{\\sum_{i = 1}^{\\ell}\n",
    "            \\lambda_i y_i}_{0}\n",
    "        +\n",
    "        \\sum_{i = 1}^{\\ell}\n",
    "            \\lambda_i\n",
    "        +\n",
    "        \\sum_{i = 1}^{\\ell}\n",
    "            \\xi_i \\underbrace{(C - \\lambda_i - \\mu_i)}_{0} \\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\lambda_i\n",
    "    -\n",
    "    \\frac{1}{2} \\sum_{i, j = 1}^{\\ell}\n",
    "        \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Мы должны потребовать выполнения условий {eq}`KuckTuckerCond2` и {eq}`KuckTuckerCond3` (если они не выполнены, то двойственная функция обращается в минус бесконечность), а также неотрицательность двойственных переменных $\\lambda_i \\geq 0$, $\\mu_i \\geq 0$. Ограничение на $\\mu_i$ и условие {eq}`KuckTuckerCond3`, можно объединить, получив $\\lambda_i \\leq C$. Приходим к следующей двойственной задаче:\n",
    "\n",
    "```{math}\n",
    ":label: svmDual\n",
    "\n",
    "\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\sum_{i = 1}^{\\ell}\n",
    "                \\lambda_i\n",
    "            -\n",
    "            \\frac{1}{2} \\sum_{i, j = 1}^{\\ell}\n",
    "                \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle\n",
    "            \\to \\max_{\\lambda} \\\\\n",
    "            & 0 \\leq \\lambda_i \\leq C, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "```\n",
    "\n",
    "Она также является вогнутой, квадратичной и имеет единственный максимум.\n",
    "\n",
    "## Ядерный переход\n",
    "\n",
    "Двойственная задача SVM {eq}`svmDual` зависит только от скалярных произведений объектов -- отдельные признаковые описания никак не входят в неё.\n",
    "\n",
    "```{note}\n",
    "Обратите внимание, как много это значит: решение SVM зависит только от скалярных произведений объектов (то есть _похожести_, если упрощать), но не от их признаковых описаний объектов. Это значит, что метод обобщается и на те случаи, когда признаковых описаний объектов нет или их получить очень дорого, но зато есть способ задать расстояние (то есть \"измерить сходство\") между объектами.\n",
    "```\n",
    "\n",
    "\n",
    "Значит, можно сделать ядерный переход:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin{aligned}\n",
    "        & \\sum_{i = 1}^{\\ell}\n",
    "            \\lambda_i\n",
    "        -\n",
    "        \\frac{1}{2} \\sum_{i, j = 1}^{\\ell}\n",
    "            \\lambda_i \\lambda_j y_i y_j K(x_i, x_j)\n",
    "        \\to \\max_{\\lambda} \\\\\n",
    "        & 0 \\leq \\lambda_i \\leq C, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "        & \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0.\n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Здесь $K(x_i, x_j)$ -- это функция-ядро, определенная на парах векторов, которая должна быть симметричной и неотрицательно определенной ([теорема Мерсера](http://www.machinelearning.ru/wiki/index.php?title=Теорема_Мерсера)).\n",
    "\n",
    "Вернемся к тому, какое представление классификатора дает двойственная задача. Из уравнения {eq}`KuckTuckerCond1` следует, что вектор весов $w$ можно представить как линейную комбинацию объектов из обучающей выборки. Подставляя это представление $w$ в классификатор, получаем\n",
    "\n",
    "$$\n",
    "a(x) = \\text{sign} \\left(\n",
    "    \\sum_{i = 1}^{\\ell} \\lambda_i y_i \\langle x_i, x \\rangle + b\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Таким образом, классификатор измеряет сходство нового объекта с объектами из обучающей выборки, вычисляя скалярное произведение между ними. Это выражение также зависит только от скалярных произведений, поэтому в нём тоже можно перейти к ядру.\n",
    "\n",
    "```{note}\n",
    "Опять подчеркнем, что классификация нового примера зависит только от скалярных произведений -- \"похожести\" нового примера на примеры из обучающей выборки, и то не все, а только опорные.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "В указанном выше представлении фигурирует переменная сдвига $b$, которая не находится непосредственно в двойственной задаче. Однако ее легко восстановить по любому граничному опорному объекту $x_i$, для которого выполнено $\\xi_i = 0, 0 < \\lambda_i < C$. Для него выполнено $y_i \\left(\\langle w, x_i \\rangle + b \\right) = 1$, откуда получаем\n",
    "\n",
    "$$\n",
    "    b = y_i - \\langle w, x_i \\rangle.\n",
    "$$\n",
    "\n",
    "Как правило, для численной устойчивости берут медиану данной величины по\n",
    "всем граничным опорным объектам:\n",
    "\n",
    "$$\n",
    "    b = med \\ y_i - \\langle w, x_i \\rangle, \\xi_i = 0, 0 < \\lambda_i < C.\n",
    "$$\n",
    "```\n",
    "\n",
    "{numref}`KernelTrick` -- пожалуй, самый известный рисунок в контексте SVM, он иллюстрирует ядерный трюк, в свою очередь, одну из самых красивых идей в истории машинного обучения. За счет ядерного перехода можно достигнуть линейной разделимости выборки даже в том случае, когда исходная обучающая выборка не является линейно разделимой.\n",
    "\n",
    "```{figure} /_static/qsvmblock/kernel_trick_idea.png\n",
    ":width: 600px\n",
    ":name: KernelTrick\n",
    "\n",
    "Пример разделимости в новом пространстве\n",
    "```\n",
    "\n",
    "Наиболее часто используемые ядра:\n",
    "\n",
    "- _Линейное_ $K(x, y) = \\langle x , y \\rangle$ -- по сути, линейный SVM, рассмотренный выше;\n",
    "- _Полиномиальное ядро_ $K(x, y) = (\\langle x , y \\rangle + c)^d$, определенное для степени ядра $d$ и параметра нормализации $c$;\n",
    "- _Гауссово ядро_, также известное как RBF (radial-basis functions) $K(x, y) = e^{-\\frac{||x - y||^2}{\\sigma}}$ c параметром ядра $\\sigma$.\n",
    "\n",
    "\n",
    "## Плюсы и минусы SVM\n",
    "\n",
    "Плюсы:\n",
    "\n",
    "- хорошо изучены, есть важные теоретические результаты;\n",
    "- красиво формулируется как задача оптимизации;\n",
    "- линейный SVM быстрый, может работать на очень больших выборках;\n",
    "- линейный SVM так же хорошо интерпретируется, как и прочие линейные модели;\n",
    "- решение зависит только от скалярных произведений векторов, а идея \"ядерного трюка\" -- одно из самых красивых в истории машинного обучения;\n",
    "- нелинейный SVM обобщается на работу с самыми разными типами данных (последовательности, графы и т.д.) за счет специфичных ядер.\n",
    "\n",
    "\n",
    "Минусы:\n",
    "\n",
    "- нелинейный SVM имеет высокую вычислительную сложность и принципиально плохо масштабируется (оптимизационную задачу нельзя \"решить на подвыборках\" и как-то объединить решения);\n",
    "- нелинейный SVM по сути не интерпретируется (\"black box\");\n",
    "- в задачах классификации часто хочется выдать вероятность отнесения к классу, SVM это не умеет делать, а эвристики, как правило, приводят к плохо откалиброванным вероятностям;\n",
    "- ядерный SVM уступает специфичным нейронным сетям уже во многих задачах, например, в в приложениях к графам"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}