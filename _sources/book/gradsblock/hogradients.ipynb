{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3e7d04",
   "metadata": {},
   "source": [
    "(hogradients)=\n",
    "\n",
    "# Градиенты высших порядков\n",
    "\n",
    "## План лекции\n",
    "\n",
    "В этой лекции мы посмотрим на ту математику, которая лежит \"под капотом\" у _parameter-shift rule_. Мы познакомимся с обобщением _parameter shift_, а также увидим, как можно оптимизировать этот метод. В конце мы узнаем, как можно посчитать производную второго порядка за минимальное количество обращений к квантовому компьютеру.\n",
    "\n",
    "Для более детального погружения в вопрос можно сразу рекомендовать статью {cite}`hogradients`.\n",
    "\n",
    "## Важность гейтов вращений\n",
    "\n",
    "Если задуматься, то одним из основных (если не единственных) способов сделать параметризованную квантовую схему является использование гейтов вращений, таких как $\\hat{RX}, \\hat{RY}, \\hat{RZ}$. Более формально это можно выразить так, что нас больше всего интересуют операторы вида:\n",
    "\n",
    "$$\n",
    "U(\\theta) = e^{-\\frac{i}{2}H\\theta}\n",
    "$$\n",
    "\n",
    "где $H$ -- оператор \"вращения\", который удовлетворяет условию $H^2 = \\mathbf{1}$. Другой возможный вариант записи -- представить матрицу $H$ как линейную комбинацию операторов Паули $\\sigma^x, \\sigma^y, \\sigma^z$.\n",
    "\n",
    "Если представить схему, содержащую множество параметризованных операторов, то итоговая запись имеет вид:\n",
    "\n",
    "$$\n",
    "U_{j...k} = U_j, ..., U_k \\ket{\\Psi}\n",
    "$$\n",
    "\n",
    "## Производная от измерения\n",
    "\n",
    "Давайте вспомним, как выглядит квантово-классическая схема обучения с **VQC**.\n",
    "\n",
    "```{figure} /_static/vqcblock/vqc/diagram.png\n",
    ":name: quantclassical\n",
    ":height: 400px\n",
    "\n",
    "Квантово-классическая схема\n",
    "```\n",
    "\n",
    "Видно, что мы хотим считать производную не от самой параметризованной схемы $U_{j...k}$, а от наблюдаемой. Для тех, кто забыл, что такое _наблюдаемая_, рекомендуем вернуться к [лекции про кубит](qubit). Если кратко, то это тот оператор, который мы \"измеряем\" на нашем квантовом компьютере. Математически производная, которая нам интересна, может быть записана для выбранного параметра $i$ таким образом:\n",
    "\n",
    "$$\n",
    "G_i = \\frac{\\partial \\bra{U_{j...k}\\Psi}\\hat{M}\\ket{U_{j...k}\\Psi}}{\\partial \\theta_i}\n",
    "$$\n",
    "\n",
    "То есть нам важно посчитать производную от результата измерения, так как именно результат измерения у нас будет определять \"предсказание\" нашей квантовой нейронной сети. Причем нам нужно уметь считать производную от любого параметра $\\theta_i$ в цепочке $\\theta_j, ...\\theta_i, ...\\theta_k$.\n",
    "\n",
    "## Parameter-shift для гейтов Паули\n",
    "\n",
    "```{note}\n",
    "Тут мы для простоты предложим, что $U_1$ это просто оператор вращения, иначе выкладки станут совсем сложными.\n",
    "```\n",
    "\n",
    "Тогда сам оператор $U_i$ может быть также записан так:\n",
    "\n",
    "$$\n",
    "U_i = e^{-\\frac{i}{2}P_i\\theta_i}\n",
    "$$\n",
    "\n",
    "Запишем результат математического ожидания через состояние $\\Psi_i$, которое пришло на вход $i$-го гейта в нашей последовательности:\n",
    "\n",
    "$$\n",
    "\\langle M(\\theta) \\rangle = Tr(M U_{k, ..., 1} \\rho_i U_{k, ..., 1}^\\dagger)\n",
    "$$\n",
    "\n",
    "где $\\rho$ это матрица плотности ($\\ket{\\Psi}\\bra{\\Psi}$). Подробнее о матрицах плотности можно почитать в ранней продвинутой лекции про смешанные состояния.\n",
    "\n",
    "Тогда частная производная от математического ожидания по $i$-му параметру $\\theta_i$ записывается (подробнее в {cite}`parametershift`) через коммутатор исходного состояния $\\rho$, которое \"пришло\" на вход гейта $U_i$ и того оператора Паули $P_i$, который мы используем в $U_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\langle M \\rangle}{\\partial \\theta_i} = -\\frac{i}{2}Tr(M U_{k, ..., i}[P_i, U_{i-1, ..., 1}\\rho_i U_{i-1, ..., 1}^\\dagger]U_{k, ..., i}^\\dagger)\n",
    "$$\n",
    "\n",
    "Этот коммутатор может быть переписан следующим образом:\n",
    "\n",
    "$$\n",
    "[P_i, \\rho] = i[U_i \\Bigl( \\frac{\\pi}{2} \\Bigr ) \\rho_i U_i^\\dagger \\Bigl ( \\frac{\\pi}{2} \\Bigr ) - U_i \\Bigl( -\\frac{\\pi}{2} \\Bigr ) \\rho_i U_i^\\dagger \\Bigl ( -\\frac{\\pi}{2} \\Bigr )]\n",
    "$$\n",
    "\n",
    "Тогда соответствующий градиент $\\frac{\\partial}{\\partial \\theta_i}$ можно записать через смещения на $\\pm\\frac{\\pi}{2}$:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\frac{\\partial \\langle M \\rangle}{\\partial \\theta_i} = \\frac{\\langle M_i^+ \\rangle - \\langle M_i^- \\rangle}{2} \\\\\n",
    "\\langle M_i^{\\pm} \\rangle = \\frac{1}{2} Tr [M U_{k, ..., i+1} U_i(\\pm \\frac{\\pi}{2})\\rho_i^` U_i^\\dagger (\\pm \\frac{\\pi}{2}) U_{k, ..., i+1}^\\dagger] \\\\\n",
    "\\rho_i^` = U_{j, ..., 1} \\rho_i U_{j, ..., 1}^\\dagger\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "По аналогии с классическими нейронными сетями и _backpropagation_ (для тех, кто забыл это понятие, рекомендуем вернуться к вводным лекциями про классическое машинное обучение) тут явно можно выделить _forward_ проход со смещением $\\theta_i$ на значения $\\frac{\\pi}{2}$ и _backward_ со смещением на $-\\frac{\\pi}{2}$.\n",
    "\n",
    "## Обобщенный parameter-shift\n",
    "\n",
    "Предложенное в {cite}`parametershift` выражение может быть на самом деле получено в более общем виде из других соображений. Так, выражение для нашей наблюдаемой $\\langle M \\rangle$ может всегда быть представлено {cite}`hogradients` как сумма вида:\n",
    "\n",
    "$$\n",
    "\\bra{U_i(\\theta_i)}\\hat{M}\\ket{U_i(\\theta_i)} = \\hat{A} + \\hat{B}\\cos{\\theta_i} + \\hat{C}\\sin{\\theta_i}\n",
    "$$\n",
    "\n",
    "где $\\hat{A}, \\hat{B}, \\hat{C}$ -- операторы, не зависящие от параметра $\\theta_i$.\n",
    "\n",
    "```{note}\n",
    "Действительно, явно выписав выражение для наблюдаемой и вспомнив формулы для косинуса и синуса двойного угла, а также воспользовавшись тем, что $U(\\theta) = e^{-\\frac{1}{2}H\\theta} = \\cos{\\frac{\\theta}{2}}\\mathbf{1} - i\\sin{\\frac{\\theta}{2}}H$, получаем:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "(\\cos{\\frac{\\theta}{2}}\\mathbf{1} + i\\sin{\\frac{\\theta}{2}}H)\\hat{M}(\\cos{\\frac{\\theta}{2}}\\mathbf{1} - i\\sin{\\frac{\\theta}{2}}H) = \\\\\n",
    "\\cos^2{\\frac{\\theta}{2}}\\hat{M} + i \\sin{\\frac{\\theta}{2}}\\cos{\\frac{\\theta}{2}}H\\hat{M} - i \\sin {\\frac{\\theta}{2}}\\cos{\\frac{\\theta}{2}}\\hat{M}H + \\sin^2{\\frac{\\theta}{2}}H\\hat{M}H = \\\\\n",
    "\\frac{1}{2} \\cos{\\theta}\\hat{M}+\\frac{1}{2}\\hat{M}+\\frac{i}{2} \\sin{\\theta}H\\hat{M} - \\frac{i}{2}\\sin{\\theta}\\hat{M}H + \\frac{1}{2}H\\hat{M}H - \\frac{1}{2}\\cos{\\theta}H\\hat{M}H = \\\\\n",
    "\\frac{1}{2}(\\hat{M} + H\\hat{M}H) + \\frac{1}{2}(\\hat{M} - H\\hat{M}H)\\cos{\\theta} + \\frac{i}{2}(H\\hat{M} - \\hat{M}H)\\sin{\\theta}\n",
    "\\end{gathered}\n",
    "$$\n",
    "```\n",
    "\n",
    "Тогда можно воспользоваться правилами тригонометрии, а именно, тем что для любого $s \\neq k\\pi, \\text{   } k \\in {1, 2, ..., }$ справедливо:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\frac{d \\cos {\\theta}}{d \\theta} = \\frac{\\cos (\\theta + s) - \\cos (\\theta - s)}{2\\sin{s}} \\\\\n",
    "\\frac{d \\sin {\\theta}}{d \\theta} = \\frac{\\sin (\\theta + s) - \\sin (\\theta - s)}{2\\sin{s}} \\\\\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "И подставим это в выражение для $\\frac{\\partial \\langle M \\rangle}{\\partial \\theta_i}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\langle M(\\theta_i) \\rangle}{\\partial \\theta_i} = \\frac{\\langle M(\\theta_i + s) \\rangle - \\langle M(\\theta_i - s) \\rangle}{2\\sin{s}}\n",
    "$$\n",
    "\n",
    "Легко заметить, что подстановка сюда $s = \\frac{\\pi}{2}$ дает нам классический _parameter shift_, описанный в {cite}`parametershift`.\n",
    "\n",
    "Наконец, запишем полученное выражение в более удобном виде, который позволит нам более эффективно выписывать производные высших порядков. Для этого введем вектор $\\mathbf{e_i}$ -- единичный вектор для $i$-го параметра, то есть вектор, где все компоненты кроме $i$-й равны нулю, а $i$-я равна 1. Тогда наше финальное выражение для обобщенного _parameter shift_ примет следующий вид:\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial f(\\mathbf{\\theta})}{\\partial \\theta_i} = \\frac{f(\\mathbf{\\theta} + s\\mathbf{e_i}) - f(\\mathbf{\\theta} - s\\mathbf{e_i})}{2\\sin{s}}}\n",
    "$$\n",
    "\n",
    "## Вторая производная и гессиан\n",
    "\n",
    "В классической теории оптимизации, также как и в машинном обучении, очень часто на первый план выходят так называемые методы 2-го порядка. Эти методы похожи на обычный градиентный спуск, но для ускорения сходимости они также используют информацию из матрицы вторых производных, которая называется гессианом. Более подробно про методы 2-го порядка и гессиан можно посмотреть в вводных лекциях курса.\n",
    "\n",
    "Методы второго порядка требуют больше вызовов, чтобы вычислить гессиан, но взамен они обеспечивают гораздо лучшую сходимость, а также менее склонны \"застревать\" в локальных минимумах. Это обеспечивает, в итоге, более быстрое обучение. В классических нейронных сетях вычисление гессиана это часто проблема, так как это матрица размерности $\\sim O(N^2)$, где $N$ -- число весов нейронной сети, и эта матрица получается слишком большой. Но, как мы помним, основная \"фича\" **VQC** это их экспоненциальная экспрессивность -- возможность линейным числом параметров (и гейтов) обеспечить преобразование, эквивалентное экспоненциальному числу весов классической нейронной сети. А значит, для них проблема размерности гессиана не стоит так остро. При этом использование гессиана теоретически позволит в итоге обучить **VQC** за меньшее число вызовов. Именно поэтому методы второго порядка потенциально очень интересны в квантово-классическом обучении. Но для начала нам необходимо разобраться, как именно можно посчитать матрицу вторых производных.\n",
    "\n",
    "Пользуясь обобщенным правилом _parameter shift_, можно выписать выражение для второй производной {cite}`hogradients`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial_2 f}{\\partial \\theta_i \\theta_j} = \\frac{f(\\mathbf{\\theta} + s_1\\mathbf{e_i} + s_2\\mathbf{e_j}) + f(\\mathbf{\\theta} - s_1\\mathbf{e_i} - s_2 \\mathbf{e_j}) - f(\\mathbf{\\theta} + s_1 \\mathbf{e_i} - s_2 \\mathbf{e_j}) - f(\\mathbf{\\theta} - s_1 \\mathbf{e_i} + s_2 \\mathbf{e_j})}{4\\sin{s_1}\\sin{s_2}}\n",
    "$$\n",
    "\n",
    "Взяв $s_1 = s_2$, можно упростить это выражение к следующему виду:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\frac{f(\\mathbf{\\theta} + s\\mathbf{a}) + f(\\mathbf{\\theta} + s\\mathbf{b}) - f(\\mathbf{\\theta} + s\\mathbf{c}) - f(\\mathbf{\\theta} + s\\mathbf{d})}{(2\\sin{s}) ^2} \\\\\n",
    "\\mathbf{a} = \\mathbf{e_i} + \\mathbf{e_j} \\\\\n",
    "\\mathbf{b} = -\\mathbf{e_i} - \\mathbf{e_j} \\\\\n",
    "\\mathbf{c} = \\mathbf{e_i} - \\mathbf{e_j} \\\\\n",
    "\\mathbf{d} = -\\mathbf{e_i} + \\mathbf{e_j}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Но чаще всего нам необходимо не просто посчитать гессиан, а еще и посчитать градиент, так как в большинстве методов 2-го порядка требуются оба эти значения. В этом случае хочется попробовать подобрать такое значение для $s_g$ при вычислении вектора градиента, а также такое значение $s_h$ при вычислении гессиана, чтобы максимально переиспользовать результаты квантовых вызовов и уменьшить их общее количество.\n",
    "\n",
    "Внимательно взглянув на выражение для 2-х производных, можно заметить, что оптимизация там возможна при расчете диагональных элементов гессиана. Давайте выпишем выражение для диагонального элемента явно:\n",
    "\n",
    "$$\n",
    "\\frac{f(\\mathbf{\\theta} + 2s\\mathbf{e_i}) + f(\\mathbf{\\theta} - 2s\\mathbf{e_i}) - 2 f(\\mathbf{\\theta})}{(2\\sin{s})^2}\n",
    "$$\n",
    "\n",
    "Можно заметить, что, например, использование $s = \\frac{\\pi}{4}$ для гессиана, а также \"стандартного\" $s=\\frac{\\pi}{2}$ для градиента позволит полностью переиспользовать в диагональных элементах гессиана значения, которые мы получили при расчете градиента. А значение $f(\\mathbf{\\theta})$ вообще считается один раз для всех диагональных вызовов.\n",
    "\n",
    "```{note}\n",
    "На самом деле, диагональные элементы гессиана можно использовать и сами по себе, например для квазиньютоновских методов оптимизации, где матрица Гессе аппроксимируется какой-то другой матрицей, чтобы не считать все вторые производные. Например, она может быть аппроксимирована диагональной матрицой, как в работе {cite}`diagonalQNewton`.\n",
    "```\n",
    "\n",
    "## Заключение\n",
    "\n",
    "В этой лекции мы познакомились с классическим _parameter shift rule_, а также его обобщением. Также мы узнали, как можно посчитать гессиан **VQC**, и даже узнали маленькие хитрости, которые можно применять для уменьшения общего количества вызовов квантовой схемы."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}